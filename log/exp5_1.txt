| distributed init (rank 0): env://, gpu 0
Namespace(aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=1024, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/yangtao/data/imagenet', data_set='IMNET', device='cuda', disable_eval=False, dist_backend='nccl', dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=True, drop_path=0.1, enable_wandb=False, epochs=300, eval=False, eval_data_path=None, finetune='', gpu=0, head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=0, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='convnext_gcc_tiny', model_ema=True, model_ema_decay=0.9999, model_ema_eval=True, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='/checkpoint/yangtao/convnext/exp5', pin_mem=True, project='convnext', rank=0, recount=1, remode='pixel', reprob=0.25, resplit=False, resume='', save_ckpt=True, save_ckpt_freq=1, save_ckpt_num=3, seed=0, smoothing=0.1, start_epoch=0, train_interpolation='bicubic', update_freq=4, use_amp=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, world_size=1)
Transform = 
RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=PIL.Image.BICUBIC)
RandomHorizontalFlip(p=0.5)
<timm.data.auto_augment.RandAugment object at 0x7fc446d72eb0>
ToTensor()
Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))
<timm.data.random_erasing.RandomErasing object at 0x7fc446d72a90>
---------------------------
reading from datapath /home/yangtao/data/imagenet
Number of the class = 1000
Transform = 
Resize(size=256, interpolation=bicubic)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/yangtao/data/imagenet
Number of the class = 1000
Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc446d72520>
Mixup is activated!
Using EMA with decay = 0.99990000
Model = ConvNeXt_gcc(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 48, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(48, 96, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=48, out_features=192, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=192, out_features=48, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=48, out_features=192, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=192, out_features=48, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(48, 48, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=48)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=48, out_features=192, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=192, out_features=48, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (6): gcc_Block(
        (pre_Norm_1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (pre_Norm_2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ffn): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (2): Hardswish()
          (3): Dropout(p=0.0, inplace=False)
          (4): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (5): Dropout(p=0.1, inplace=False)
        )
        (ca): CA_layer(
          (gap): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Hardswish()
            (3): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): Hardsigmoid()
          )
        )
      )
      (7): gcc_Block(
        (pre_Norm_1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (pre_Norm_2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ffn): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (2): Hardswish()
          (3): Dropout(p=0.0, inplace=False)
          (4): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (5): Dropout(p=0.1, inplace=False)
        )
        (ca): CA_layer(
          (gap): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Hardswish()
            (3): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): Hardsigmoid()
          )
        )
      )
      (8): gcc_Block(
        (pre_Norm_1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (pre_Norm_2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ffn): Sequential(
          (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (2): Hardswish()
          (3): Dropout(p=0.0, inplace=False)
          (4): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))
          (5): Dropout(p=0.1, inplace=False)
        )
        (ca): CA_layer(
          (gap): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Conv2d(192, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Hardswish()
            (3): Conv2d(12, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): Hardsigmoid()
          )
        )
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): gcc_Block(
        (pre_Norm_1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (pre_Norm_2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (ffn): Sequential(
          (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (2): Hardswish()
          (3): Dropout(p=0.0, inplace=False)
          (4): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))
          (5): Dropout(p=0.1, inplace=False)
        )
        (ca): CA_layer(
          (gap): AdaptiveAvgPool2d(output_size=1)
          (fc): Sequential(
            (0): Conv2d(384, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): Hardswish()
            (3): Conv2d(24, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): Hardsigmoid()
          )
        )
      )
    )
  )
  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=384, out_features=1000, bias=True)
)
number of params: 5919376
LR = 0.00400000
Batch size = 4096
Update frequent = 4
Number of training examples = 1281167
Number of training training per epoch = 312
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.meta_kernel_1_H",
      "stages.2.6.meta_kernel_1_W",
      "stages.2.6.meta_kernel_2_H",
      "stages.2.6.meta_kernel_2_W",
      "stages.2.6.meta_pe_1_H",
      "stages.2.6.meta_pe_1_W",
      "stages.2.6.meta_pe_2_H",
      "stages.2.6.meta_pe_2_W",
      "stages.2.6.ffn.1.weight",
      "stages.2.6.ffn.4.weight",
      "stages.2.6.ca.fc.0.weight",
      "stages.2.6.ca.fc.3.weight",
      "stages.2.7.meta_kernel_1_H",
      "stages.2.7.meta_kernel_1_W",
      "stages.2.7.meta_kernel_2_H",
      "stages.2.7.meta_kernel_2_W",
      "stages.2.7.meta_pe_1_H",
      "stages.2.7.meta_pe_1_W",
      "stages.2.7.meta_pe_2_H",
      "stages.2.7.meta_pe_2_W",
      "stages.2.7.ffn.1.weight",
      "stages.2.7.ffn.4.weight",
      "stages.2.7.ca.fc.0.weight",
      "stages.2.7.ca.fc.3.weight",
      "stages.2.8.meta_kernel_1_H",
      "stages.2.8.meta_kernel_1_W",
      "stages.2.8.meta_kernel_2_H",
      "stages.2.8.meta_kernel_2_W",
      "stages.2.8.meta_pe_1_H",
      "stages.2.8.meta_pe_1_W",
      "stages.2.8.meta_pe_2_H",
      "stages.2.8.meta_pe_2_W",
      "stages.2.8.ffn.1.weight",
      "stages.2.8.ffn.4.weight",
      "stages.2.8.ca.fc.0.weight",
      "stages.2.8.ca.fc.3.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.meta_kernel_1_H",
      "stages.3.2.meta_kernel_1_W",
      "stages.3.2.meta_kernel_2_H",
      "stages.3.2.meta_kernel_2_W",
      "stages.3.2.meta_pe_1_H",
      "stages.3.2.meta_pe_1_W",
      "stages.3.2.meta_pe_2_H",
      "stages.3.2.meta_pe_2_W",
      "stages.3.2.ffn.1.weight",
      "stages.3.2.ffn.4.weight",
      "stages.3.2.ca.fc.0.weight",
      "stages.3.2.ca.fc.3.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.meta_1_H_bias",
      "stages.2.6.meta_1_W_bias",
      "stages.2.6.meta_2_H_bias",
      "stages.2.6.meta_2_W_bias",
      "stages.2.6.pre_Norm_1.weight",
      "stages.2.6.pre_Norm_1.bias",
      "stages.2.6.pre_Norm_2.weight",
      "stages.2.6.pre_Norm_2.bias",
      "stages.2.6.ffn.0.weight",
      "stages.2.6.ffn.0.bias",
      "stages.2.6.ffn.1.bias",
      "stages.2.6.ffn.4.bias",
      "stages.2.6.ca.fc.1.weight",
      "stages.2.6.ca.fc.1.bias",
      "stages.2.6.ca.fc.4.weight",
      "stages.2.6.ca.fc.4.bias",
      "stages.2.7.meta_1_H_bias",
      "stages.2.7.meta_1_W_bias",
      "stages.2.7.meta_2_H_bias",
      "stages.2.7.meta_2_W_bias",
      "stages.2.7.pre_Norm_1.weight",
      "stages.2.7.pre_Norm_1.bias",
      "stages.2.7.pre_Norm_2.weight",
      "stages.2.7.pre_Norm_2.bias",
      "stages.2.7.ffn.0.weight",
      "stages.2.7.ffn.0.bias",
      "stages.2.7.ffn.1.bias",
      "stages.2.7.ffn.4.bias",
      "stages.2.7.ca.fc.1.weight",
      "stages.2.7.ca.fc.1.bias",
      "stages.2.7.ca.fc.4.weight",
      "stages.2.7.ca.fc.4.bias",
      "stages.2.8.meta_1_H_bias",
      "stages.2.8.meta_1_W_bias",
      "stages.2.8.meta_2_H_bias",
      "stages.2.8.meta_2_W_bias",
      "stages.2.8.pre_Norm_1.weight",
      "stages.2.8.pre_Norm_1.bias",
      "stages.2.8.pre_Norm_2.weight",
      "stages.2.8.pre_Norm_2.bias",
      "stages.2.8.ffn.0.weight",
      "stages.2.8.ffn.0.bias",
      "stages.2.8.ffn.1.bias",
      "stages.2.8.ffn.4.bias",
      "stages.2.8.ca.fc.1.weight",
      "stages.2.8.ca.fc.1.bias",
      "stages.2.8.ca.fc.4.weight",
      "stages.2.8.ca.fc.4.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.meta_1_H_bias",
      "stages.3.2.meta_1_W_bias",
      "stages.3.2.meta_2_H_bias",
      "stages.3.2.meta_2_W_bias",
      "stages.3.2.pre_Norm_1.weight",
      "stages.3.2.pre_Norm_1.bias",
      "stages.3.2.pre_Norm_2.weight",
      "stages.3.2.pre_Norm_2.bias",
      "stages.3.2.ffn.0.weight",
      "stages.3.2.ffn.0.bias",
      "stages.3.2.ffn.1.bias",
      "stages.3.2.ffn.4.bias",
      "stages.3.2.ca.fc.1.weight",
      "stages.3.2.ca.fc.1.bias",
      "stages.3.2.ca.fc.4.weight",
      "stages.3.2.ca.fc.4.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
Set warmup steps = 6240
Set warmup steps = 0
Max WD = 0.0500000, Min WD = 0.0500000
criterion = SoftTargetCrossEntropy()
Auto resume checkpoint: 
Start training for 300 epochs
Epoch: [0]  [   0/1251]  eta: 9:08:31  lr: 0.000000  min_lr: 0.000000  loss: 7.0624 (7.0624)  weight_decay: 0.0500 (0.0500)  time: 26.3082  data: 22.9373  max mem: 57946
Epoch: [0]  [  10/1251]  eta: 1:29:04  lr: 0.000001  min_lr: 0.000001  loss: 7.0656 (7.0672)  weight_decay: 0.0500 (0.0500)  time: 4.3069  data: 2.0856  max mem: 57946
Epoch: [0]  [  20/1251]  eta: 1:06:41  lr: 0.000003  min_lr: 0.000003  loss: 7.0679 (7.0696)  weight_decay: 0.0500 (0.0500)  time: 2.0975  data: 0.0005  max mem: 57946
Epoch: [0]  [  30/1251]  eta: 0:58:21  lr: 0.000004  min_lr: 0.000004  loss: 7.0626 (7.0686)  weight_decay: 0.0500 (0.0500)  time: 2.0761  data: 0.0058  max mem: 57946
Epoch: [0]  [  40/1251]  eta: 0:53:55  lr: 0.000006  min_lr: 0.000006  loss: 7.0623 (7.0679)  weight_decay: 0.0500 (0.0500)  time: 2.0646  data: 0.0109  max mem: 57946
Epoch: [0]  [  50/1251]  eta: 0:51:03  lr: 0.000008  min_lr: 0.000008  loss: 7.0691 (7.0685)  weight_decay: 0.0500 (0.0500)  time: 2.0605  data: 0.0055  max mem: 57946
Epoch: [0]  [  60/1251]  eta: 0:49:03  lr: 0.000010  min_lr: 0.000010  loss: 7.0603 (7.0660)  weight_decay: 0.0500 (0.0500)  time: 2.0610  data: 0.0052  max mem: 57946
Epoch: [0]  [  70/1251]  eta: 0:47:29  lr: 0.000011  min_lr: 0.000011  loss: 7.0564 (7.0647)  weight_decay: 0.0500 (0.0500)  time: 2.0588  data: 0.0052  max mem: 57946
Epoch: [0]  [  80/1251]  eta: 0:46:14  lr: 0.000013  min_lr: 0.000013  loss: 7.0573 (7.0637)  weight_decay: 0.0500 (0.0500)  time: 2.0570  data: 0.0054  max mem: 57946
Epoch: [0]  [  90/1251]  eta: 0:45:11  lr: 0.000014  min_lr: 0.000014  loss: 7.0561 (7.0624)  weight_decay: 0.0500 (0.0500)  time: 2.0620  data: 0.0094  max mem: 57946
Epoch: [0]  [ 100/1251]  eta: 0:44:16  lr: 0.000016  min_lr: 0.000016  loss: 7.0422 (7.0602)  weight_decay: 0.0500 (0.0500)  time: 2.0607  data: 0.0045  max mem: 57946
Epoch: [0]  [ 110/1251]  eta: 0:43:28  lr: 0.000017  min_lr: 0.000017  loss: 7.0377 (7.0583)  weight_decay: 0.0500 (0.0500)  time: 2.0638  data: 0.0006  max mem: 57946
Epoch: [0]  [ 120/1251]  eta: 0:42:44  lr: 0.000019  min_lr: 0.000019  loss: 7.0333 (7.0562)  weight_decay: 0.0500 (0.0500)  time: 2.0623  data: 0.0005  max mem: 57946
Epoch: [0]  [ 130/1251]  eta: 0:42:04  lr: 0.000021  min_lr: 0.000021  loss: 7.0290 (7.0536)  weight_decay: 0.0500 (0.0500)  time: 2.0625  data: 0.0058  max mem: 57946
Epoch: [0]  [ 140/1251]  eta: 0:41:26  lr: 0.000022  min_lr: 0.000022  loss: 7.0290 (7.0518)  weight_decay: 0.0500 (0.0500)  time: 2.0629  data: 0.0059  max mem: 57946
Epoch: [0]  [ 150/1251]  eta: 0:40:51  lr: 0.000024  min_lr: 0.000024  loss: 7.0206 (7.0491)  weight_decay: 0.0500 (0.0500)  time: 2.0573  data: 0.0006  max mem: 57946
Epoch: [0]  [ 160/1251]  eta: 0:40:17  lr: 0.000026  min_lr: 0.000026  loss: 7.0087 (7.0466)  weight_decay: 0.0500 (0.0500)  time: 2.0554  data: 0.0006  max mem: 57946
Epoch: [0]  [ 170/1251]  eta: 0:39:46  lr: 0.000027  min_lr: 0.000027  loss: 7.0063 (7.0441)  weight_decay: 0.0500 (0.0500)  time: 2.0686  data: 0.0113  max mem: 57946
Epoch: [0]  [ 180/1251]  eta: 0:39:19  lr: 0.000029  min_lr: 0.000029  loss: 6.9936 (7.0412)  weight_decay: 0.0500 (0.0500)  time: 2.0993  data: 0.0171  max mem: 57946
Epoch: [0]  [ 190/1251]  eta: 0:38:49  lr: 0.000030  min_lr: 0.000030  loss: 6.9906 (7.0382)  weight_decay: 0.0500 (0.0500)  time: 2.0875  data: 0.0111  max mem: 57946
Epoch: [0]  [ 200/1251]  eta: 0:38:20  lr: 0.000032  min_lr: 0.000032  loss: 6.9817 (7.0355)  weight_decay: 0.0500 (0.0500)  time: 2.0654  data: 0.0056  max mem: 57946
Epoch: [0]  [ 210/1251]  eta: 0:37:52  lr: 0.000033  min_lr: 0.000033  loss: 6.9764 (7.0325)  weight_decay: 0.0500 (0.0500)  time: 2.0661  data: 0.0008  max mem: 57946
Epoch: [0]  [ 220/1251]  eta: 0:37:25  lr: 0.000035  min_lr: 0.000035  loss: 6.9690 (7.0296)  weight_decay: 0.0500 (0.0500)  time: 2.0701  data: 0.0071  max mem: 57946
Epoch: [0]  [ 230/1251]  eta: 0:36:59  lr: 0.000037  min_lr: 0.000037  loss: 6.9662 (7.0268)  weight_decay: 0.0500 (0.0500)  time: 2.0708  data: 0.0072  max mem: 57946
Epoch: [0]  [ 240/1251]  eta: 0:36:32  lr: 0.000038  min_lr: 0.000038  loss: 6.9621 (7.0241)  weight_decay: 0.0500 (0.0500)  time: 2.0651  data: 0.0047  max mem: 57946
